# -*- coding: utf-8 -*-
"""
Code related to the transformed text-embedder layer:
Used to transform the 768-dimension token embeddings generated by Bert to 512-dimensions  

Expected input is:
[batch_number, token_number, token_embeddings (768d)]

Returns:
[batch_number, token_number, transformed embeddings (512d]
 
"""

# Commented out IPython magic to ensure Python compatibility.
import pdb
import torch.nn as nn
from .util import PositionalEncoding

"""### Text Embedder"""
class TextEmbedder(nn.Module):
    def __init__(self, input_size=512, hidden_size=512, max_len=5000, dropout=0.3):
      super(TextEmbedder,self).__init__()
      self.input_size = input_size
      self.hidden_size = hidden_size
      self.dropout = dropout
      
      self.embedding = nn.Embedding(num_embeddings=30522, embedding_dim=input_size, padding_idx=0)
      self.position_encoding = PositionalEncoding(embedding_size=hidden_size, max_len=max_len)
      self.ffn = nn.Sequential(
            nn.Linear(in_features=input_size, out_features=hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size),
          ) 

    def forward(self, x):
      embeddings = self.embedding(x)
      x = self.ffn(embeddings)
      return  self.position_encoding(x.permute(1,0,2)).permute(1,0,2)